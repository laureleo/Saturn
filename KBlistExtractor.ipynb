{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uppgift\n",
    "Skriv ett parallelliserat program som går igenom ett antal listor med strängar (varje lista finns i en separat indatafil). Programmet ska producera en ny lista innehållande alla strängar ur indatalistorna som börjar med bokstaven 'r'. Den producerade listan ska vara fri från dubbletter. Utgå från att listorna skulle kunna vara väldigt långa (fastän din faktiska indata är liten). Du kan anta att strängarna i listorna är korta och antalet listor är litet samt att den slutgiltiga listan blir kort.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['utf8list3.txt',\n",
       " 'utf8list2.txt',\n",
       " 'utf8list0.txt',\n",
       " 'utf8list6.txt',\n",
       " 'MOCKutf8list12.txt',\n",
       " 'utf8list8.txt',\n",
       " 'MOCKutf8list10.txt',\n",
       " 'utf8list4.txt',\n",
       " 'utf8list1.txt',\n",
       " 'utf8list7.txt',\n",
       " 'MOCKutf8list14.txt',\n",
       " 'MOCKutf8list13.txt',\n",
       " 'MOCKutf8list11.txt',\n",
       " 'utf8list9.txt',\n",
       " 'utf8list5.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assetPath = \"../assets/KBlistExtractorAssets/\"\n",
    "filenames = os.listdir(assetPath)\n",
    "filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesser():\n",
    "    with open(assetPath + 'merged.txt', 'wb') as outfile:\n",
    "        for fname in filenames:\n",
    "            with open(assetPath + fname, 'rb') as infile:\n",
    "                for line in infile:\n",
    "                    outfile.write(line)\n",
    "#preprocesser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_pandas(file, character_to_filter_with = 'r',):\n",
    "    filepath = assetPath + file\n",
    "    file_data = pd.read_csv(filepath, names = ['strings'])\n",
    "    filtered_data = file_data[file_data['strings'].str[0] == character_to_filter_with]\n",
    "    return list(filtered_data['strings'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rgyvruqtppxufio', 'rgyvruqtppxufio', 'rxojwrpqtninpmt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_data(file, character_to_filter_with = 'r',):\n",
    "    filepath = assetPath + file\n",
    "    \n",
    "    strings = []\n",
    "    \n",
    "    with open(filepath, 'r', encoding ='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line[0] == 'r':\n",
    "                strings.append(line[:-1])\n",
    "\n",
    "    return strings\n",
    "extract_data(filenames[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_byte(file, character_to_filter_with = 'r',):\n",
    "    filepath = assetPath + file\n",
    "    \n",
    "    strings = []\n",
    "    \n",
    "    with open(filepath, 'rb') as f:\n",
    "        for line in f:\n",
    "            if line[:1] == b'r':\n",
    "                strings.append(line)\n",
    "                \n",
    "    return strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_byte_to_string(file, character_to_filter_with = 'r',):\n",
    "    filepath = assetPath + file\n",
    "    \n",
    "    strings = []\n",
    "    \n",
    "    with open(filepath, 'rb') as f:\n",
    "        for line in f:\n",
    "            if line[:1] == b'r':\n",
    "                strings.append(line.decode(\"utf-8\")[:-1])\n",
    "                \n",
    "    return strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_byte_to_string_by_set(file, character_to_filter_with = 'r',):\n",
    "    filepath = assetPath + file\n",
    "    \n",
    "    strings = set()\n",
    "    \n",
    "    with open(filepath, 'rb') as f:\n",
    "        for line in f:\n",
    "            if line[:1] == b'r':\n",
    "                strings.update([line.decode(\"utf-8\")[:-1]])\n",
    "                \n",
    "    return strings\n",
    "#extract_data_byte_to_string_by_set(filenames[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trivial(filenames, dataextraction = extract_data):\n",
    "    results = []\n",
    "    for file in filenames:\n",
    "        filtered_data =  dataextraction(file)\n",
    "        for string in filtered_data:\n",
    "            results.append(string)\n",
    "\n",
    "    return list(set(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rxojwrpqtninpmt',\n",
       " 'rxdadoolpdqcoih',\n",
       " 'rgyvruqtppxufio',\n",
       " 'ryinnpkkppfixjy',\n",
       " 'rqeiqokxvnqqugv',\n",
       " 'rsghlhjwdukrtqy',\n",
       " 'rakxschavfumwqi',\n",
       " 'rdfbljbxnjvotty',\n",
       " 'rblkxwxoumiparc',\n",
       " 'rilolmptmcjhhxn',\n",
       " 'rfyegiaceaquwib',\n",
       " 'rppoabafuatnxly',\n",
       " 'rnyffrvrsluvssg',\n",
       " 'roudtyqpixcjnfu']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def trivial_partial_set(filenames, dataextraction = extract_data_byte_to_string_by_set):\n",
    "    results = set()\n",
    "    for file in filenames:\n",
    "        filtered_data =  dataextraction(file)\n",
    "        results.update(filtered_data)\n",
    "\n",
    "    return list(results)\n",
    "trivial_partial_set(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    " def parallel(filenames, dataextraction = extract_data):\n",
    "    pool = mp.Pool()\n",
    "\n",
    "    r = pool.map_async(dataextraction, filenames)\n",
    "\n",
    "\n",
    "    results = []\n",
    "    for partial_result in r.get():\n",
    "        for string in partial_result:\n",
    "            results.append(string)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    return list(set(results))\n",
    "\n",
    "#parallel(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " def parallel_partial_set(filenames, dataextraction = extract_data_byte_to_string_by_set):\n",
    "    pool = mp.Pool()\n",
    "\n",
    "    r = pool.map_async(dataextraction, filenames)\n",
    "    \n",
    "    results = set()\n",
    "    for partial_result in r.get():\n",
    "        for string in partial_result:\n",
    "            results.update([string])\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    return list(results)\n",
    "\n",
    "#parallel_partial_set(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.474238872528076"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_function_execution_time(iterations, function, *args):\n",
    "    start_time = time.time()\n",
    "    for i in range(iterations):\n",
    "        function(*args)\n",
    "    return (time.time() - start_time)/iterations\n",
    "\n",
    "\n",
    "get_function_execution_time(1, trivial,filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concurrent, pandas. \n",
      "Time: 4.7372682094573975\n",
      "Concurrent, normal. \n",
      "Time: 2.3750832080841064\n",
      "Concurrent, bytes. \n",
      "Time: 0.9587936401367188\n",
      "Concurrent, bytes->string. \n",
      "Time: 0.9404957294464111\n",
      "Concurrent, bytes->string. using sets instead of concatenating lists\n",
      "Time: 0.9525682926177979\n"
     ]
    }
   ],
   "source": [
    "ex_speed = get_function_execution_time(iterations, trivial, filenames, extract_data_pandas)\n",
    "print(f\"Concurrent, pandas. \\nTime: {ex_speed}\")\n",
    "\n",
    "ex_speed = get_function_execution_time(iterations, trivial, filenames, extract_data)\n",
    "print(f\"Concurrent, normal. \\nTime: {ex_speed}\")\n",
    "\n",
    "ex_speed = get_function_execution_time(iterations, trivial, filenames, extract_data_byte)\n",
    "print(f\"Concurrent, bytes. \\nTime: {ex_speed}\")\n",
    "\n",
    "ex_speed = get_function_execution_time(iterations, trivial, filenames, extract_data_byte_to_string)\n",
    "print(f\"Concurrent, bytes->string. \\nTime: {ex_speed}\")\n",
    "\n",
    "ex_speed = get_function_execution_time(iterations, trivial_partial_set, filenames, extract_data_byte_to_string_by_set)\n",
    "print(f\"Concurrent, bytes->string. using sets instead of concatenating lists\\nTime: {ex_speed}\")\n",
    "\n",
    "#ex_speed = get_function_execution_time(iterations, trivial, ['merged.txt'], extract_data_byte_to_string)\n",
    "#print(f\"Concurrent, bytes->string with preprocessing. \\nTime: {ex_speed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parallel, pandas. \n",
      "Time: 1.2301671504974365\n",
      "parallel, normal. \n",
      "Time: 0.6332626342773438\n",
      "parallel, bytes. \n",
      "Time: 0.33527398109436035\n",
      "parallel, bytes->string. \n",
      "Time: 0.33889126777648926\n",
      "Parallel, bytes->string. using sets instead of concatenating lists\n",
      "Time: 0.33441853523254395\n"
     ]
    }
   ],
   "source": [
    "ex_speed = get_function_execution_time(iterations, parallel, filenames, extract_data_pandas)\n",
    "print(f\"parallel, pandas. \\nTime: {ex_speed}\")\n",
    "\n",
    "ex_speed = get_function_execution_time(iterations, parallel, filenames, extract_data)\n",
    "print(f\"parallel, normal. \\nTime: {ex_speed}\")\n",
    "\n",
    "ex_speed = get_function_execution_time(iterations, parallel, filenames, extract_data_byte)\n",
    "print(f\"parallel, bytes. \\nTime: {ex_speed}\")\n",
    "    \n",
    "ex_speed = get_function_execution_time(iterations, parallel, filenames, extract_data_byte_to_string)\n",
    "print(f\"parallel, bytes->string. \\nTime: {ex_speed}\")\n",
    "\n",
    "ex_speed = get_function_execution_time(iterations, parallel_partial_set, filenames, extract_data_byte_to_string_by_set)\n",
    "print(f\"Parallel, bytes->string. using sets instead of concatenating lists\\nTime: {ex_speed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the amount of data in the files\n",
    "def create_fake_data():\n",
    "    possible_strings = []\n",
    "    for file in filenames:\n",
    "        filepath = assetPath + file\n",
    "        with open(filepath, 'r') as f:\n",
    "            for line in f:\n",
    "                possible_strings.append(line[:-1])\n",
    "\n",
    "    long_list = possible_strings\n",
    "    for i in range(15):\n",
    "        long_list.extend(long_list)\n",
    "    print(f\"Created list of length {len(long_list)}\")\n",
    "    \n",
    "    mock_data = [\n",
    "    \"MOCKutf8list10.txt\",\n",
    "    \"MOCKutf8list11.txt\",\n",
    "    \"MOCKutf8list12.txt\",\n",
    "    \"MOCKutf8list13.txt\",\n",
    "    \"MOCKutf8list14.txt\"\n",
    "    ]\n",
    "\n",
    "    for name in mock_data:\n",
    "        with open(assetPath + name, 'w') as f:\n",
    "            print(f\"wrote {name}\")\n",
    "            f.writelines(long_list)\n",
    "#create_fake_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check amount of running processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9465\t0\t1617430\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bashCommand = \"cat /proc/sys/fs/file-nr\"\n",
    "process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\n",
    "output, error = process.communicate()\n",
    "print(output.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook KBlistExtractor.ipynb to script\n",
      "[NbConvertApp] Writing 7276 bytes to KBlistExtractor.py\n"
     ]
    }
   ],
   "source": [
    "#!jupyter nbconvert --to script KBlistExtractor.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
