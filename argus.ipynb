{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Argus\n",
    "\n",
    "Scope:\n",
    "* Sweden:\n",
    "* Scandinavia:\n",
    "* Europe:\n",
    "* World\n",
    "\n",
    "Scrape the main news for each country.\n",
    "\n",
    "Do I need a manual approach for each newspaper? Tiresome. Let's try to automate things.\n",
    "Grab the text element with the largest font on the main page?\n",
    "\n",
    "Grab the elements, search for h1...h6, pick the first largest one (and the subtext?)?\n",
    "\n",
    "No, font size is better.\n",
    "\n",
    "## Links\n",
    "https://en.wikipedia.org/wiki/List_of_newspapers_in_Sweden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DONE\n",
    "https://www.svd.se/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure\n",
    "\n",
    "Each scraper will have:\n",
    "* URL\n",
    "* Source Country\n",
    "* Source Language\n",
    "* Source Newspaper\n",
    "*\n",
    "* Scrape_function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pycountry\n",
    "from googletrans import Translator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mind_of_argus:\n",
    "    \"\"\"\n",
    "    The Mind of Argus processes what all eyes see...\n",
    "    This class takes in a destination language and a list of eyes.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, target_lang = 'en'):\n",
    "        self.target_lang = target_lang\n",
    "        self.translator = Translator()\n",
    "    \n",
    "\n",
    "    def communicate(self, url):\n",
    "        eye = eye_of_argus(url)\n",
    "        header = eye.perspective['header']\n",
    "        excerpt = eye.perspective['excerpt']\n",
    "        \n",
    "        print(f\"{eye.country}: {eye.newspaper}\")\n",
    "\n",
    "        trans_header = self.translator.translate(header, dest = self.target_lang).text\n",
    "        trans_excerpt = self.translator.translate(excerpt, dest = self.target_lang).text\n",
    "        print(trans_header)\n",
    "        print(trans_excerpt)\n",
    "        print()\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "a = mind_of_argus()\n",
    "\n",
    "\n",
    "\n",
    "places = [\n",
    "'https://www.aftonbladet.se/',\n",
    "'https://www.svd.se/',\n",
    "'https://www.gp.se/',\n",
    "'https://www.dn.se/',\n",
    "'https://www.expressen.se/',\n",
    "'https://politiken.dk/',\n",
    "'https://www.spiegel.de/',\n",
    "'https://www.lemonde.fr/'\n",
    "         \n",
    "]\n",
    "m = mind_of_argus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Auto): Sweden: (Auto): aftonbladet\n",
      "Mass sales when the Swedes are corona bunkers\n",
      "▸ Become a hard currency ✓ \"Out of stock, out of stock\"\n",
      "(Auto): Sweden: (Auto): svd\n",
      "Australia demands formal apology from Beijing\n",
      "Prime Minister Scott Morrison calls China's behavior \"disgusting.\"\n",
      "(Auto): Sweden: (Auto): gp\n",
      "After suspected copper thefts - entire department is replaced\n",
      "Gothenburg Trams' CEO Hans Nilsson: \"We simply need to start over\"\n",
      "(Auto): Sweden: (Auto): dn\n",
      "Russia is fighting a devastating second wave\n",
      "Death toll rises sharply - but the Kremlin wants to avoid a new shutdown.\n",
      "(Auto): Sweden: (Auto): expressen\n",
      "Large fire in reservoirs - risk of spreading\n",
      "Magazine overheated in Bjuv - close to other buildings\n",
      "(Auto): Denmark: (Auto): https://politiken\n",
      "New restrictions in the capital: The spread of viruses among children and young people must go down\n",
      "Not Found\n",
      "(Auto): Germany: (Auto): spiegel\n",
      "Marketplace\n",
      "DISPLAY\n",
      "(Auto): France: (Auto): lemonde\n",
      "Services\n",
      "Service: Conforama promo codes: free shipping from € 300 -10% for students with ASOS Groupon: -20% for new customers\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for p in places:\n",
    "    m.communicate(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Service: vocational training Discover our distance learning courses Learn to code online Take your skills assessment online'"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Translator.translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.content)\n",
    "    return soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "class eye_of_argus():\n",
    "    \"\"\"\n",
    "    The Eyes of Argus sees all.\n",
    "    \n",
    "    The basic scraper class for this project.\n",
    "    Each instance of an eye should at least provide an url, with which the eye will do it's best to get relevant content.\n",
    "    Specifying the type of scraper, newspaper name etc make the returned information more relevant\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, url, country = 'auto', newspaper = 'auto', language = 'auto', notes = 'None'):\n",
    "        self.url = url\n",
    "        self.country = country\n",
    "        self.newspaper = newspaper\n",
    "        self.language = language\n",
    "        self.notes = notes\n",
    "        self.perspective = self.get_perspective()\n",
    "        self.format_perspective()\n",
    "        \n",
    "        if self.newspaper == 'auto':\n",
    "            self.newspaper =  '(Auto): ' + self.url.split(sep = '.')[-2]\n",
    "        \n",
    "        if self.country =='auto':\n",
    "            domain = self.url.split(sep = '.')[-1][:-1] \n",
    "            try:\n",
    "                country = pycountry.countries.get(alpha_2=domain).name\n",
    "                self.country = '(Auto): ' + country\n",
    "            except:\n",
    "                self.country = \"Failed to infer\"\n",
    "            \n",
    "    def get_soup(self):\n",
    "        html = requests.get(self.url)\n",
    "        soup = BeautifulSoup(html.content)\n",
    "        return soup\n",
    "\n",
    "\n",
    "    def get_perspective(self):\n",
    "        content = self.get_soup()\n",
    "\n",
    "        response = {\n",
    "        'header' : \"Not Found\",\n",
    "        'excerpt' : \"Not Found\"\n",
    "        }\n",
    "        \n",
    "        if self.url not in manual.keys():\n",
    "            response = self.naive_scrape(content, response)\n",
    "        \n",
    "        else:\n",
    "            eye = manual[self.url]\n",
    "            response = self.smart_scraper(content, response, eye)\n",
    "\n",
    "        return response\n",
    "    \n",
    "    def format_perspective(self):\n",
    "        #Make sure the content is a string. If not, assume it is a bs4 tag and extract the text element\n",
    "        for key in self.perspective:\n",
    "            \n",
    "            if self.perspective[key] == '\\n':\n",
    "                self.perspective[key] = \"Not Found\"\n",
    "                \n",
    "            if type(self.perspective[key]) != type('string'):\n",
    "                self.perspective[key] = self.perspective[key].text\n",
    "            \n",
    "            # Remove leading and trailing whitespace\n",
    "            split = self.perspective[key].split()\n",
    "            join = ' '.join(split)\n",
    "            self.perspective[key] = join\n",
    "            \n",
    "    def naive_scrape(self, content, response):\n",
    "        for head in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "            rubric = content.find(head)\n",
    "            if rubric != None:\n",
    "                response['header'] = rubric\n",
    "                response['excerpt'] = rubric.findNext()\n",
    "                if response['excerpt'] == '\\n':\n",
    "                    response['excerpt'] = rubric.findNext().findNext()\n",
    "                    \n",
    "        return response\n",
    "    \n",
    "    def smart_scraper(self, content, response, specific_scraper):\n",
    "        \"\"\"\n",
    "        The smart_scraper is a higher-order function\n",
    "        It relies on the existence of functions with a specific format.\n",
    "        That is, a function that retures the header element of content when 'header' is specified'\n",
    "        and the excerpt element when 'excerpt is specified'\n",
    "        \"\"\"\n",
    "    \n",
    "        header = specific_scraper(content, 'header')\n",
    "        if header != None:\n",
    "            response['header'] = header\n",
    "            excerpt = specific_scraper(content, 'excerpt')\n",
    "            if excerpt != None:\n",
    "                response['excerpt'] =  excerpt\n",
    "        return response\n",
    "   \n",
    "    def describe(self, level = 0):\n",
    "        auto = ''\n",
    "        if self.url not in manual.keys():\n",
    "            auto = '(Auto):'\n",
    "\n",
    "        if level == 0:\n",
    "\n",
    "            print(f\"\"\"\n",
    "            SOURCE: {self.newspaper}\n",
    "            Header: {auto} {self.perspective['header']}\n",
    "\n",
    "            Excerpt: {auto} {self.perspective['excerpt']}\n",
    "            \"\"\")\n",
    "        else:\n",
    "            \n",
    "            print(f\"\"\"\n",
    "            url: {self.url}\n",
    "            country: {self.country}\n",
    "            newspaper: {self.newspaper}\n",
    "            language: {self.language}\n",
    "            notes: {self.notes}\n",
    "\n",
    "            Header:{auto} {self.perspective['header']}\n",
    "\n",
    "            Excerpt: {auto} {self.perspective['excerpt']}\n",
    "            \"\"\")\n",
    "            \n",
    "manual = {\n",
    "    'https://www.aftonbladet.se/' : swe_aft,\n",
    "    'https://www.svd.se/' : swe_svd,\n",
    "    'https://www.gp.se/': swe_gp,\n",
    "    'https://www.dn.se/': swe_dn,\n",
    "    'https://www.expressen.se/': swe_exp,\n",
    "    'https://politiken.dk/': dk_pol\n",
    "      \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# GERMANY\n",
    "\n",
    "# DENMARK\n",
    "def dk_pol(content, item):\n",
    "    if item == 'header':\n",
    "        return content.find('h2', attrs={'class': 'article-intro__title headline headline--xxxlarge'})\n",
    "    if item == 'excerpt':\n",
    "        return content.find('h2', attrs={'class': 'article-intro__title headline headline--xxxlarge'}).parent.find('ul', attrs={'class': 'article-intro__related'})\n",
    "\n",
    "# SWEDEN\n",
    "\n",
    "def swe_exp(content, item):\n",
    "    if item == 'header':\n",
    "        return content.find('div', attrs={'class': 'teaser'}).find('h2')\n",
    "    if item == 'excerpt':\n",
    "        return content.find('div', attrs={'class': 'teaser'}).find('h2').next_sibling\n",
    "\n",
    "def swe_dn(content, item):\n",
    "    if item == 'header':\n",
    "        return content.find('div', attrs= {'class': 'teaser-package__content'}).find('h1')\n",
    "    if item == 'excerpt':\n",
    "        return content.find('div', attrs= {'class': 'teaser-package__content'}).find('h1').next_sibling.next_sibling\n",
    "\n",
    "def swe_gp(content, item):\n",
    "    if item == 'header':\n",
    "        return content.find(\"div\",  attrs = {'class': 'c-teaser__content'}).find(\"h2\", attrs = {'class': 'c-teaser__title'})\n",
    "    if item == 'excerpt':\n",
    "        return content.find(\"div\", attrs = {'class': 'c-teaser__summery'})\n",
    "    \n",
    "def swe_aft(content, item):\n",
    "    if item == 'header':\n",
    "        return content.find('h3')\n",
    "    if item == 'excerpt':\n",
    "        return content.find('h3').next_sibling\n",
    "    \n",
    "def swe_svd(content, item):\n",
    "    if item == 'header':\n",
    "        return content.find('h2')\n",
    "    if item == 'excerpt':\n",
    "        return content.find('h2').next_sibling.next_sibling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
